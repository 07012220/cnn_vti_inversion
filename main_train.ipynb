{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is used for the neural network construction and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.utils import multi_gpu_model, plot_model\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras import regularizers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_folder = './train_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure fonts \n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lists\n",
    "with open('list_dataset_filepaths', 'rb') as fp:\n",
    "    list_dataset_filepaths = pickle.load(fp) # list of the paths to samples form the dataset\n",
    "with open('list_parameters', 'rb') as fp:\n",
    "    list_parameters= pickle.load(fp) # list of parameters (neural network output)\n",
    "# load coefficient shot gather (input) normalization\n",
    "with open('max_seism_value', 'rb') as fp:\n",
    "    max_seism_value = pickle.load(fp) # this parameter is used for the input dataset normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_size = len(list_dataset_filepaths)\n",
    "assert len(list_dataset_filepaths) == len(list_parameters)\n",
    "print('datset size:', datset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading block\n",
    "# function for the dataset reading from file \n",
    "filename_r_time = './dataset10/seism_time.bin' # each sample (shot gather) has the same time size, which is saved in this file\n",
    "time_full = np.fromfile (filename_r_time)\n",
    "mean_timestep = len (time_full)\n",
    "\n",
    "num_of_rec_in_group = 13\n",
    "epoch_number = 0 # the value changes during training process\n",
    "def read_x_data(list_dataset_filepaths):\n",
    "#     np.random.seed()\n",
    "    global epoch_number, time_full, mean_timestep\n",
    "    seismogram = np.zeros((len(list_dataset_filepaths), num_of_rec_in_group, mean_timestep))\n",
    "#     amp_map = np.zeros((num_of_rec_in_group, mean_timestep)) # amplitude map (moving average) fot noise adding\n",
    "#     N = 220 # width of the window usded for the amplitude map calculation\n",
    "    gain = np.exp(-4e5*time_full[:]**2)*1e2/(epoch_number+1)+1 #epoch_number=500 in the end of training\n",
    "    gain /= max_seism_value\n",
    "    for ifile, file_path in enumerate(list_dataset_filepaths):\n",
    "        filename_r = file_path\n",
    "        seism_read = np.fromfile(filename_r)\n",
    "        for irec in range(num_of_rec_in_group):\n",
    "            seismogram[ifile, irec, :] = seism_read[irec*mean_timestep:(irec+1)*mean_timestep]*gain\n",
    "#             amp_map[irec, :] = np.convolve(abs(seismogram[ifile, irec, :]), np.ones((N))/N, mode='same')\n",
    "#         noise = np.random.rand(num_of_rec_in_group, mean_timestep)/5-0.1\n",
    "#         seismogram[ifile, :, :] = seismogram[ifile, :, :] + noise*amp_map[:,:]\n",
    "        if (seismogram.shape[1]+8)*seismogram.shape[2]*8 != os.path.getsize(filename_r):\n",
    "            print('error! smth wrong with reading')\n",
    "    return seismogram.reshape(seismogram.shape[0], seismogram.shape[1],seismogram.shape[2], 1) #channels last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion used for the Keras fit_generator\n",
    "def dataset_loader(list_dataset_filepaths, list_parameters, batch_size):\n",
    "    L=len(list_dataset_filepaths)\n",
    "    #this line is just to make the generator infinite, keras needs that\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            x_dataset = read_x_data(list_dataset_filepaths[batch_start:limit])\n",
    "            y_dataset = np.array(list_parameters[batch_start:limit])\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "            yield (x_dataset, y_dataset) #a tuple with two numpy arrays with batch_size samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output array (desired parameters) has to be normalized\n",
    "def normalize_list_parameters(list_parameters):\n",
    "    if datset_size != len(list_parameters):\n",
    "        print('error! smth wrong with dataset size')\n",
    "    list_parameters_numpy = np.asarray(list_parameters, dtype=np.float32)\n",
    "    rho_max = np.max(list_parameters_numpy[:,0])\n",
    "    vp_max = np.max(list_parameters_numpy[:,1])\n",
    "    vs_max = np.max(list_parameters_numpy[:,2])\n",
    "    eps_max = np.max(list_parameters_numpy[:,3])\n",
    "    gamma_max = np.max(list_parameters_numpy[:,4])\n",
    "    delta_max = np.max(list_parameters_numpy[:,5])\n",
    "    list_parameters_numpy[:,0] /= rho_max\n",
    "    list_parameters_numpy[:,1] /= vp_max\n",
    "    list_parameters_numpy[:,2] /= vs_max\n",
    "    list_parameters_numpy[:,3] /= eps_max\n",
    "    list_parameters_numpy[:,4] /= gamma_max\n",
    "    list_parameters_numpy[:,5] /= delta_max\n",
    "    rho_mean = np.mean(list_parameters_numpy[:,0])\n",
    "    vp_mean = np.mean(list_parameters_numpy[:,1])\n",
    "    vs_mean = np.mean(list_parameters_numpy[:,2])\n",
    "    eps_mean = np.mean(list_parameters_numpy[:,3])\n",
    "    gamma_mean = np.mean(list_parameters_numpy[:,4])\n",
    "    delta_mean = np.mean(list_parameters_numpy[:,5])\n",
    "    list_parameters_numpy[:,0] -= rho_mean\n",
    "    list_parameters_numpy[:,1] -= vp_mean\n",
    "    list_parameters_numpy[:,2] -= vs_mean\n",
    "    list_parameters_numpy[:,3] -= eps_mean\n",
    "    list_parameters_numpy[:,4] -= gamma_mean\n",
    "    list_parameters_numpy[:,5] -= delta_mean\n",
    "    list_parameters_normalized = [] \n",
    "    for i in range(datset_size):\n",
    "        list_parameters_normalized.append( [ list_parameters_numpy[i,0], list_parameters_numpy[i,1], list_parameters_numpy[i,2], list_parameters_numpy[i,3], list_parameters_numpy[i,4], list_parameters_numpy[i,5] ] )\n",
    "    return list_parameters_normalized, rho_max, vp_max, vs_max, eps_max, gamma_max, delta_max, rho_mean, vp_mean, vs_mean, eps_mean, gamma_mean, delta_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalization coefficients to the file\n",
    "# we will need when using trained neural network\n",
    "list_parameters, rho_max, vp_max, vs_max, eps_max, gamma_max, delta_max, rho_mean, vp_mean, vs_mean, eps_mean, gamma_mean, delta_mean = normalize_list_parameters(list_parameters)\n",
    "normalization_param_list = []\n",
    "normalization_param_list.append('rho_max='+'{}'.format(rho_max)+'\\n')\n",
    "normalization_param_list.append('vp_max='+'{}'.format(vp_max)+'\\n')\n",
    "normalization_param_list.append('vs_max='+'{}'.format(vs_max)+'\\n')\n",
    "normalization_param_list.append('eps_max='+'{}'.format(eps_max)+'\\n')\n",
    "normalization_param_list.append('gamma_max='+'{}'.format(gamma_max)+'\\n')\n",
    "normalization_param_list.append('delta_max='+'{}'.format(delta_max)+'\\n')\n",
    "normalization_param_list.append('rho_mean='+'{}'.format(rho_mean)+'\\n')\n",
    "normalization_param_list.append('vp_mean='+'{}'.format(vp_mean)+'\\n')\n",
    "normalization_param_list.append('vs_mean='+'{}'.format(vs_mean)+'\\n')\n",
    "normalization_param_list.append('eps_mean='+'{}'.format(eps_mean)+'\\n')\n",
    "normalization_param_list.append('gamma_mean='+'{}'.format(gamma_mean)+'\\n')\n",
    "normalization_param_list.append('delta_mean='+'{}'.format(delta_mean)+'\\n')\n",
    "with open(train_output_folder + \"normalization_param_list.txt\", \"w\") as f_write:\n",
    "    for lineWrite in normalization_param_list:\n",
    "        f_write.write(lineWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and validation subsets\n",
    "np.random.seed()\n",
    "list_filepaths_train, list_filepaths_valid, true_parameters_train, true_parameters_valid = train_test_split(list_dataset_filepaths, list_parameters, test_size=0.1)\n",
    "print('train dataset size:', len(list_filepaths_train))\n",
    "print('validation dataset size:', len(list_filepaths_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check shapes of the x and y dataset\n",
    "x_dataset_example = read_x_data(list_filepaths_train[9:10])\n",
    "y_dataset_example = np.array(true_parameters_train[9:10])\n",
    "print ('x_dataset shape (batch(=1), num_of_rec_in_group, timesteps, channels(=1)):', x_dataset_example.shape)\n",
    "print ('y_dataset shape (batch(=1), dim[vp ,vs]):', y_dataset_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with tf.device('/cpu:0'):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(filters=50, input_shape=(13,5500,1), kernel_size=(6,6), strides=(1,1), padding='same', activation='relu'))\n",
    "#     model.add(Conv2D(filters=50, kernel_size=(5,5), strides=(1,1), padding='same', activation='relu'))\n",
    "#     model.add(Conv2D(filters=50, kernel_size=(5,5), strides=(1,1), padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "#     model.add(Conv2D(filters=75, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n",
    "#     model.add(Conv2D(filters=75, kernel_size=(3,3), strides=(1,2), padding='valid', activation='relu'))    \n",
    "#     model.add(Conv2D(filters=75, kernel_size=(3,3), strides=(1,2), padding='valid', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(filters=100, kernel_size=(2,2), strides=(1,2), padding='same', activation='relu'))\n",
    "#     model.add(Conv2D(filters=100, kernel_size=(2,2), strides=(1,2), padding='valid', activation='relu'))\n",
    "#     model.add(Conv2D(filters=100, kernel_size=(2,2), strides=(1,2), padding='valid', activation='relu'))\n",
    "#     model.add(Flatten())\n",
    "#     model.add( Dense(2500, activation='relu') )\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add( Dense(750, activation='relu') )\n",
    "#     model.add( Dense(200, activation='relu') )\n",
    "#     model.add( Dense(6) )\n",
    "#     model.add(Activation('linear'))\n",
    "#     print('model initialized')\n",
    "#     with open(train_output_folder + 'model_summary.txt', 'w') as file_write_sum:\n",
    "#         with redirect_stdout(file_write_sum):\n",
    "#             model.summary()\n",
    "#     model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or load pretrained model\n",
    "model = load_model(train_output_folder + 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "batch_size=16;\n",
    "nb_epoch=500;\n",
    "print('nb_epoch:', nb_epoch)\n",
    "print('steps_per_epoch:', np.ceil(datset_size/batch_size))\n",
    "print('validation_steps:', np.ceil(len(list_filepaths_valid)/batch_size))\n",
    "parallel_model = multi_gpu_model(model, gpus=4)\n",
    "parallel_model.compile(loss='mean_squared_error', optimizer=optimizers.Adadelta())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions which gets epoch number during training process\n",
    "def get_epoch(epoch):\n",
    "    global epoch_number\n",
    "    epoch_number = epoch\n",
    "\n",
    "GetEpoch_callback = LambdaCallback(on_epoch_begin=lambda epoch,logs: get_epoch(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "start_time = time.time()\n",
    "history=parallel_model.fit_generator(dataset_loader(list_filepaths_train, true_parameters_train, batch_size), \n",
    "                                     steps_per_epoch=np.ceil(datset_size/batch_size), epochs=nb_epoch, verbose=1, \n",
    "                                     validation_data=dataset_loader(list_filepaths_valid, true_parameters_valid, batch_size), validation_steps=np.ceil(len(list_filepaths_valid)/batch_size), callbacks=[GetEpoch_callback])\n",
    "model.save(train_output_folder + 'model.h5')\n",
    "done_time = time.time()\n",
    "elapsed_time = done_time - start_time\n",
    "print('elapsed time:', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot training and validation loss function values\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "fig=plt.figure(figsize=(12, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history.history['loss'][3:], linewidth=2)\n",
    "plt.plot(history.history['val_loss'][3:],'--', linewidth=2)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "print('train_loss for the last training epoch:', history.history['loss'][-1])\n",
    "print('valid_loss for the last training epoch:', history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check predictions for the validation dataset\n",
    "predictions_valid = parallel_model.predict_generator(dataset_loader(list_filepaths_valid, true_parameters_valid, batch_size), steps=np.ceil(len(list_filepaths_valid)/batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put here normalization coefficients, if you uploaded complete model\n",
    "# rho_max=3839.0\n",
    "# vp_max=6489.0\n",
    "# vs_max=3999.0\n",
    "# eps_max=0.25956490635871887\n",
    "# gamma_max=0.24065768718719482\n",
    "# delta_max=0.2931036949157715\n",
    "# rho_mean=0.6626886129379272\n",
    "# vp_mean=0.6760891079902649\n",
    "# vs_mean=0.7519354820251465\n",
    "# eps_mean=0.30609753727912903\n",
    "# gamma_mean=0.19618447124958038\n",
    "# delta_mean=0.2660829424858093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to real values\n",
    "for i in range(len(predictions_valid)):\n",
    "    #unMEAN\n",
    "    predictions_valid[i][0] += rho_mean\n",
    "    predictions_valid[i][1] += vp_mean\n",
    "    predictions_valid[i][2] += vs_mean\n",
    "    predictions_valid[i][3] += eps_mean\n",
    "    predictions_valid[i][4] += gamma_mean\n",
    "    predictions_valid[i][5] += delta_mean\n",
    "    true_parameters_valid[i][0] += rho_mean\n",
    "    true_parameters_valid[i][1] += vp_mean\n",
    "    true_parameters_valid[i][2] += vs_mean\n",
    "    true_parameters_valid[i][3] += eps_mean\n",
    "    true_parameters_valid[i][4] += gamma_mean\n",
    "    true_parameters_valid[i][5] += delta_mean\n",
    "    #unMAX\n",
    "    predictions_valid[i][0] *= rho_max\n",
    "    predictions_valid[i][1] *= vp_max\n",
    "    predictions_valid[i][2] *= vs_max\n",
    "    predictions_valid[i][3] *= eps_max\n",
    "    predictions_valid[i][4] *= gamma_max\n",
    "    predictions_valid[i][5] *= delta_max\n",
    "    true_parameters_valid[i][0] *= rho_max\n",
    "    true_parameters_valid[i][1] *= vp_max\n",
    "    true_parameters_valid[i][2] *= vs_max\n",
    "    true_parameters_valid[i][3] *= eps_max\n",
    "    true_parameters_valid[i][4] *= gamma_max\n",
    "    true_parameters_valid[i][5] *= delta_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot predictions vs true values\n",
    "true_parameters_valid = np.array(true_parameters_valid)\n",
    "predictions_valid = np.array(predictions_valid)\n",
    "#one_png\n",
    "fig_res, ax_res = plt.subplots(6,1)\n",
    "fig_res.set_size_inches(10, 50)\n",
    "ax_res[0].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$\\rho, kg/m^3$')\n",
    "ax_res[0].scatter(true_parameters_valid[:,0], predictions_valid[:,0], facecolors='none', edgecolors='b')\n",
    "ax_res[0].locator_params(nbins=6)\n",
    "ax_res[1].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$V_{p_0}, m/s$')\n",
    "ax_res[1].scatter(true_parameters_valid[:,1], predictions_valid[:,1], facecolors='none', edgecolors='b')\n",
    "ax_res[1].locator_params(nbins=6)\n",
    "ax_res[2].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$V_{s_0}, m/s$')\n",
    "ax_res[2].scatter(true_parameters_valid[:,2], predictions_valid[:,2], facecolors='none', edgecolors='b')\n",
    "ax_res[2].locator_params(nbins=6)\n",
    "ax_res[3].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$\\varepsilon$')\n",
    "ax_res[3].scatter(true_parameters_valid[:,3], predictions_valid[:,3], facecolors='none', edgecolors='b')\n",
    "ax_res[3].locator_params(nbins=6)\n",
    "ax_res[4].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$\\gamma$')\n",
    "ax_res[4].scatter(true_parameters_valid[:,4], predictions_valid[:,4], facecolors='none', edgecolors='b')\n",
    "ax_res[4].locator_params(nbins=6)\n",
    "ax_res[5].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$\\delta$')\n",
    "ax_res[5].scatter(true_parameters_valid[:,5], predictions_valid[:,5], facecolors='none', edgecolors='b')\n",
    "ax_res[5].locator_params(nbins=6)\n",
    "plt.savefig(train_output_folder + 'predictions_all.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
