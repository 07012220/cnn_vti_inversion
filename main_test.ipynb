{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_folder = './train_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonts\n",
    "import matplotlib\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lists\n",
    "with open('list_dataset_filepaths', 'rb') as fp:\n",
    "    list_dataset_filepaths = pickle.load(fp) # list of the paths to samples form the dataset\n",
    "with open('list_parameters', 'rb') as fp:\n",
    "    list_parameters= pickle.load(fp) # list of parameters (neural network output)\n",
    "# load coefficient shot gather (input) normalization\n",
    "with open('max_seism_value', 'rb') as fp:\n",
    "    max_seism_value = pickle.load(fp) # this parameter is used for the input dataset normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_size = len(list_dataset_filepaths)\n",
    "assert len(list_dataset_filepaths) == len(list_parameters)\n",
    "print('datset size:', datset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading block\n",
    "# function for the dataset reading from file \n",
    "filename_r_time = './dataset10/seism_time.bin' # each sample (shot gather) has the same time size, which is saved in this file\n",
    "time_full = np.fromfile (filename_r_time)\n",
    "mean_timestep = len (time_full)\n",
    "\n",
    "num_of_rec_in_group = 13\n",
    "epoch_number = 500 # the value changes during training process\n",
    "def read_x_data(list_dataset_filepaths):\n",
    "    np.random.seed()\n",
    "    global epoch_number, time_full, mean_timestep\n",
    "    seismogram = np.zeros((len(list_dataset_filepaths), num_of_rec_in_group, mean_timestep))\n",
    "#     amp_map = np.zeros((num_of_rec_in_group, mean_timestep)) # amplitude map (moving average) fot noise adding\n",
    "#     N = 220 # width of the window usded for the amplitude map calculation\n",
    "#     gain = np.exp(-4e5*time_full[:]**2)*1e2/(epoch_number+1)+1\n",
    "    gain = np.exp(-4e5*time_full[:]**2)*1e2/(epoch_number+1)+1\n",
    "    gain /= max_seism_value\n",
    "    for ifile, file_path in enumerate(list_dataset_filepaths):\n",
    "        filename_r = file_path\n",
    "        seism_read = np.fromfile(filename_r)\n",
    "        for irec in range(num_of_rec_in_group):\n",
    "            seismogram[ifile, irec, :] = seism_read[irec*mean_timestep:(irec+1)*mean_timestep]*gain\n",
    "#             amp_map[irec, :] = np.convolve(abs(seismogram[ifile, irec, :]), np.ones((N))/N, mode='same')\n",
    "#         noise = np.random.rand(num_of_rec_in_group, mean_timestep)/5-0.1\n",
    "#         seismogram[ifile, :, :] = seismogram[ifile, :, :] + noise*amp_map[:,:]\n",
    "        if (seismogram.shape[1]+8)*seismogram.shape[2]*8 != os.path.getsize(filename_r):\n",
    "            print('error! smth wrong with reading')\n",
    "    return seismogram.reshape(seismogram.shape[0], seismogram.shape[1],seismogram.shape[2], 1) #channels last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion used for the Keras fit_generator\n",
    "def dataset_loader(list_dataset_filepaths, list_parameters, batch_size):\n",
    "    L=len(list_dataset_filepaths)\n",
    "    #this line is just to make the generator infinite, keras needs that\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            x_dataset = read_x_data(list_dataset_filepaths[batch_start:limit])\n",
    "            y_dataset = np.array(list_parameters[batch_start:limit])\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size\n",
    "            yield (x_dataset, y_dataset) #a tuple with two numpy arrays with batch_size samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shapes of the x and y dataset\n",
    "# y dataset is needed here only for checking\n",
    "x_dataset_example = read_x_data(list_dataset_filepaths[9:10])\n",
    "y_dataset_example = np.array(list_parameters[9:10])\n",
    "print ('x_dataset shape (batch(=1), num_of_rec_in_group, timesteps, channels(=1)):', x_dataset_example.shape)\n",
    "print ('y_dataset shape (batch(=1), dim[vp ,vs]):', y_dataset_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = load_model(train_output_folder + 'model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "predictions = model.predict_generator(dataset_loader(list_dataset_filepaths, list_parameters, batch_size), steps=np.ceil(len(list_dataset_filepaths)/batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from normalization_param_list.txt\n",
    "rho_max=3839.0\n",
    "vp_max=6489.0\n",
    "vs_max=3999.0\n",
    "eps_max=0.25956490635871887\n",
    "gamma_max=0.24065768718719482\n",
    "delta_max=0.2931036949157715\n",
    "rho_mean=0.6626886129379272\n",
    "vp_mean=0.6760891079902649\n",
    "vs_mean=0.7519354820251465\n",
    "eps_mean=0.30609753727912903\n",
    "gamma_mean=0.19618447124958038\n",
    "delta_mean=0.2660829424858093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to real values\n",
    "# take these coef from normilize_log_file.ipynb\n",
    "for i in range(len(predictions)):\n",
    "    #unMEAN\n",
    "    predictions[i][0] += rho_mean\n",
    "    predictions[i][1] += vp_mean\n",
    "    predictions[i][2] += vs_mean\n",
    "    predictions[i][3] += eps_mean\n",
    "    predictions[i][4] += gamma_mean\n",
    "    predictions[i][5] += delta_mean\n",
    "    #unMAX\n",
    "    predictions[i][0] *= rho_max\n",
    "    predictions[i][1] *= vp_max\n",
    "    predictions[i][2] *= vs_max\n",
    "    predictions[i][3] *= eps_max\n",
    "    predictions[i][4] *= gamma_max\n",
    "    predictions[i][5] *= delta_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('pred:', predictions[0])\n",
    "# print('true:', list_parameters[0])\n",
    "list_parameters = np.array(list_parameters)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_bad_points(arr1, arr2):\n",
    "#     arr_bad = abs(arr1-arr2)/arr2\n",
    "#     arr_bad_idx = np.where(arr_bad>0.09)\n",
    "#     arr_bad = arr_bad[arr_bad_idx]\n",
    "#     return arr_bad_idx\n",
    "\n",
    "# bad_eps_idx = find_bad_points(list_parameters[:,3], predictions[:,3])\n",
    "# print(len(bad_eps_idx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#one_png\n",
    "fig_res, ax_res = plt.subplots(6,1)\n",
    "fig_res.set_size_inches(10, 50)\n",
    "ax_res[0].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$\\rho, kg/m^3$')\n",
    "ax_res[0].scatter(list_parameters[:,0], predictions[:,0], facecolors='none', edgecolors='b')\n",
    "ax_res[0].locator_params(nbins=6)\n",
    "ax_res[1].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$V_{p_0}, m/s$')\n",
    "ax_res[1].scatter(list_parameters[:,1], predictions[:,1], facecolors='none', edgecolors='b')\n",
    "ax_res[1].locator_params(nbins=6)\n",
    "ax_res[2].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$V_{s_0}, m/s$')\n",
    "ax_res[2].scatter(list_parameters[:,2], predictions[:,2], facecolors='none', edgecolors='b')\n",
    "ax_res[2].locator_params(nbins=6)\n",
    "ax_res[3].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$\\varepsilon$')\n",
    "ax_res[3].scatter(list_parameters[:,3], predictions[:,3], facecolors='none', edgecolors='b')\n",
    "ax_res[3].locator_params(nbins=6)\n",
    "ax_res[4].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$\\gamma$')\n",
    "ax_res[4].scatter(list_parameters[:,4], predictions[:,4], facecolors='none', edgecolors='b')\n",
    "ax_res[4].locator_params(nbins=6)\n",
    "ax_res[5].set(xlabel='Reference CNN output', ylabel= 'Calculated CNN output', title=r'$\\delta$')\n",
    "ax_res[5].scatter(list_parameters[:,5], predictions[:,5], facecolors='none', edgecolors='b')\n",
    "ax_res[5].locator_params(nbins=6)\n",
    "\n",
    "# ax_res[0].scatter(list_parameters[bad_eps_idx,0], predictions[bad_eps_idx,0], facecolors='r', edgecolors='r')\n",
    "# ax_res[1].scatter(list_parameters[bad_eps_idx,1], predictions[bad_eps_idx,1], facecolors='r', edgecolors='r')\n",
    "# ax_res[2].scatter(list_parameters[bad_eps_idx,2], predictions[bad_eps_idx,2], facecolors='r', edgecolors='r')\n",
    "# ax_res[3].scatter(list_parameters[bad_eps_idx,3], predictions[bad_eps_idx,3], facecolors='r', edgecolors='r')\n",
    "# ax_res[4].scatter(list_parameters[bad_eps_idx,4], predictions[bad_eps_idx,4], facecolors='r', edgecolors='r')\n",
    "# ax_res[5].scatter(list_parameters[bad_eps_idx,5], predictions[bad_eps_idx,5], facecolors='r', edgecolors='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
